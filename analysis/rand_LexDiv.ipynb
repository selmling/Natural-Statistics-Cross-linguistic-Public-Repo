{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e95ac1c",
   "metadata": {},
   "source": [
    "# Natural Statistics Cross-linguistic: \n",
    "\n",
    "#### Lexical diversity analysis - random sample \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, \"data_proc\")\n",
    "import analytic_proc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2ae690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSE_rand_dat_inc = pd.read_csv(\"../data/TSE_cont_dat.csv\")\n",
    "TSE_rand_dat_inc.rename(columns={'Langauge_name': 'Language_name'}, inplace=True)\n",
    "TSE_rand_dat_inc['target_child_id'] = TSE_rand_dat_inc['transcript_id']\n",
    "# only include 'Mother' as speaker_role\n",
    "TSE_rand_dat_inc = TSE_rand_dat_inc[TSE_rand_dat_inc['speaker_role'] == 'Mother']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb4a00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc = pd.read_csv(\"../data/rand_dat_inc_master.csv\",index_col=0,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59f6651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean data\n",
    "rand_dat_inc=rand_dat_inc[rand_dat_inc[\"language\"]!=\"ara\"]\n",
    "rand_dat_inc=rand_dat_inc[(rand_dat_inc[\"target_child_age\"]>=5) & (rand_dat_inc[\"target_child_age\"]<=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3459a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this way we include older and younger Tseltal kids\n",
    "rand_dat_inc = pd.concat([TSE_rand_dat_inc, rand_dat_inc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e0264f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc_cg = rand_dat_inc[rand_dat_inc[\"caregiver\"]==\"caregiver\"]\n",
    "\n",
    "rand_dat_inc_cg[\"contingent\"] = np.where(rand_dat_inc_cg[\"contingent\"]==1, \"contingent\", \"non-contingent\")\n",
    "\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"].notna()]\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"]!=\"xxx\"]\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"]!=\"yyy\"]\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"]!=\"www\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would call the following if we could run lexical diversity analysis globally:\n",
    "\n",
    "```python\n",
    "analytic_proc.create_result(rand_dat_inc_cg)\n",
    "```\n",
    "\n",
    "However, we want to have seperate dictionaries for contingent and non-contingent words so we can compare them to one another.\n",
    "\n",
    "The function will allow us to have a different dictionary for each transcript.\n",
    "\n",
    "Finally, to compare, we can run mixed effects to understand whether contingent and non-contingent utterances differ in their lexical diversity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Seperate contingent and non-contingent utterances into individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc_cg_cc = rand_dat_inc_cg[rand_dat_inc_cg[\"contingent\"]==\"contingent\"].reset_index(drop=True)\n",
    "rand_dat_inc_cg_nc = rand_dat_inc_cg[rand_dat_inc_cg[\"contingent\"]==\"non-contingent\"].reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Loop through each unique transcript to compute the lexical diversity counts across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analytic_proc.create_c_result(rand_dat_inc_cg_cc,\"rand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_proc.create_nc_result(rand_dat_inc_cg_nc,\"rand\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Lexical Diversity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc_cg_cc = pd.read_csv(\"../data/rand_dat_inc_master_cc_lexdiv.csv\",index_col=0,low_memory=False)\n",
    "rand_dat_inc_cg_nc = pd.read_csv(\"../data/rand_dat_inc_master_nc_lexdiv.csv\",index_col=0,low_memory=False)\n",
    "\n",
    "# combine dataframes into one\n",
    "\n",
    "rand_dat_inc_cg = pd.concat([rand_dat_inc_cg_cc,rand_dat_inc_cg_nc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "470b2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add play context and year of study\n",
    "\n",
    "play_context = pd.read_csv(\"../data/context_data.csv\")\n",
    "play_context = play_context.rename(columns={\"Corpus\": \"corpus_name\"})\n",
    "\n",
    "# print(play_context.to_markdown())\n",
    "\n",
    "rand_dat_inc_cg = rand_dat_inc_cg.merge(play_context,on='corpus_name')\n",
    "\n",
    "rand_dat_inc_cg[\"context\"] = rand_dat_inc_cg[\"Location\"] + rand_dat_inc_cg[\"Activity\"]\n",
    "\n",
    "rand_dat_inc_cg[\"context\"] = rand_dat_inc_cg[\"context\"].replace({\"HomeBook-reading\":\"Home: book reading\",\n",
    "                                                                 \"HomeInterview/Unstructured\":\"Home: interview/unstructured\",\n",
    "                                                                 \"HomeNaN\":\"Home: unreported\",\n",
    "                                                                 \"HomeOther\":\"Home: other\",\n",
    "                                                                 \"HomeUnstructured\":\"Home: unstructured\",\n",
    "                                                                 \"LabOther\":\"Lab: other\",\n",
    "                                                                 \"LabTabletop play\":\"Lab: tabletop play\",\n",
    "                                                                 \"LabInterview/Unstructured\":\"Lab: interview/unstructured\",\n",
    "                                                                 \"LabUnstructured\":\"Lab: unstructured\",\n",
    "                                                                 np.nan:\"Unreported\",\n",
    "                                                                 \"OtherUnstructured\":\"Other: unstructured\"})\n",
    "\n",
    "# year of study\n",
    "# corpora_year = pd.read_csv(\"../data/corpora_year.csv\")\n",
    "# corpora_year = corpora_year.rename(columns={\"Corpora\": \"corpus_name\"})\n",
    "# corpora_year = corpora_year[[\"corpus_name\", \"Year collected\"]]\n",
    "\n",
    "# rand_dat_inc_cg = rand_dat_inc_cg.merge(corpora_year,on='corpus_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c122f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rand_dat_inc_cg[\"Language_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | stat   |      age |\n",
      "|---:|:-------|---------:|\n",
      "|  0 | min    |  2       |\n",
      "|  1 | max    | 36       |\n",
      "|  2 | mean   | 19.2619  |\n",
      "|  3 | stdev  |  6.83642 |\n"
     ]
    }
   ],
   "source": [
    "# age descriptives\n",
    "ar = [['min', rand_dat_inc_cg[\"target_child_age\"].min()],\n",
    "      ['max', rand_dat_inc_cg[\"target_child_age\"].max()],\n",
    "      ['mean', rand_dat_inc_cg[\"target_child_age\"].mean()],\n",
    "      ['stdev', rand_dat_inc_cg[\"target_child_age\"].std()]]\n",
    "\n",
    "age_range = pd.DataFrame(ar, columns = ['stat', 'age'])\n",
    "\n",
    "# inspect data:\n",
    "print(age_range.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c398a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "826952b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i rand_dat_inc_cg\n",
    "\n",
    "library(\"lme4\")\n",
    "library(\"repr\")\n",
    "library(\"knitr\")\n",
    "library(\"broom\")\n",
    "library(\"emmeans\")\n",
    "library(\"tidyverse\")\n",
    "library(\"kableExtra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8624f22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining with `by = join_by(transcript_id)`\n"
     ]
    }
   ],
   "source": [
    "%%R -o rand_dat_inc_cg\n",
    "\n",
    "# ---- create caregiver type categories\n",
    "\n",
    "caregiver_type <- rand_dat_inc_cg %>%\n",
    "  group_by(transcript_id) %>%\n",
    "  summarise(\n",
    "    caregiver_type = case_when(\n",
    "      all(speaker_role == \"Mother\") ~ \"Mother only\",\n",
    "      all(speaker_role == \"Father\") ~ \"Father only\",\n",
    "      any(speaker_role %in% c(\"Mother\", \"Father\")) ~ \"Mother & Father\",\n",
    "      TRUE ~ \"Unknown\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "# inspect data:\n",
    "# caregiver_type %>%\n",
    "#     kbl(format=\"pipe\")\n",
    "    \n",
    "# ggplot(caregiver_type, aes(x = 1, y = caregiver_type, fill = factor(caregiver_type))) + \n",
    "#   geom_col() +\n",
    "#   coord_polar(theta = \"y\") +\n",
    "#   theme_void()\n",
    "\n",
    "rand_dat_inc_cg <- rand_dat_inc_cg %>%\n",
    "  left_join(caregiver_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_lex_stats = (rand_dat_inc_cg.groupby([\"Language_name\",\"target_child_id\",\"transcript_id\",\"contingent\"]) #,\"context\",\"Year collected\"])\n",
    "                                  .uniqueness\n",
    "                                  .agg([\"sum\"])\n",
    "                                  .reset_index())\n",
    "rand_lex_sumstats = rand_lex_stats.rename({'sum': 'sums'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc6fba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n"
     ]
    }
   ],
   "source": [
    "%%R -i rand_lex_sumstats\n",
    "\n",
    "# ^import rand_lex_sumstats into R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a3cd9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining with `by = join_by(transcript_id)`\n"
     ]
    }
   ],
   "source": [
    "%%R -o rand_lex_sumstats\n",
    "\n",
    "rand_lex_sumstats <- rand_lex_sumstats %>%\n",
    "    left_join(caregiver_type) %>%\n",
    "    filter(Language_name != \"Mandarin\") %>%\n",
    "    filter(Language_name != \"Polish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data:\n",
    "len(rand_lex_sumstats[\"Language_name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1586"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data:\n",
    "len(rand_lex_sumstats[\"transcript_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to file:\n",
    "rand_lex_sumstats.to_csv(\"../data/rand_lex_sumstats.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i rand_lex_sumstats\n",
    "\n",
    "options(repr.plot.width=6, repr.plot.height=12, scipen=999)\n",
    "\n",
    "xlabs <- c(\"C\", \"NC\")\n",
    "\n",
    "# ara_label <- data.frame(means=c(0),contingent = c(1.5),language=\"ara\") # no adult speech transcribed\n",
    "deu_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"German\")\n",
    "eng_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"English\")\n",
    "est_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"Estonian\")\n",
    "fas_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"Persian\")\n",
    "fra_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"French\")\n",
    "hrv_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"Croatian\")\n",
    "jpn_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"Japanese\")\n",
    "kor_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"Korean\")\n",
    "# nor_label <- data.frame(means=c(5.8),contingent = c(1.5),Language_name=\"Norwegian\")\n",
    "nor_ns_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Norwegian\")\n",
    "pol_ns_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Polish\")\n",
    "por_label <- data.frame(sums=c(240),contingent = c(1.5),Language_name=\"Portuguese\")\n",
    "spa_ns_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Spanish\")\n",
    "swe_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Swedish\")\n",
    "tse_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Tseltal\")\n",
    "# zho_ns_label <- data.frame(means=c(6),contingent = c(1.5),Language_name=\"Mandarin\")\n",
    "\n",
    "\n",
    "p <- ggplot(rand_lex_sumstats, aes(x = contingent, y = sums, color = Language_name)) +\n",
    "     stat_summary(fun.y=mean, geom=\"point\", shape=19, size=1.75) + \n",
    "     stat_summary(fun.data = mean_se, geom = \"errorbar\", size=1.25, width = .5) +\n",
    "     facet_wrap(. ~ Language_name,ncol = 7) + \n",
    "     geom_text(data = deu_label,label = \"***\",size=8,color=\"black\") + \n",
    "     geom_text(data = eng_label,label = \"***\",size=8,color=\"black\") +  \n",
    "     geom_text(data = est_label,label = \"**\",size=8,color=\"black\") +  \n",
    "#      geom_text(data = fas_ns_label,label = \"*\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "     geom_text(data = fas_label,label = \"*\",size=8, color=\"black\") +\n",
    "     geom_text(data = fra_label,label = \"***\",size=8,color=\"black\") +  \n",
    "     geom_text(data = hrv_label,label = \"***\",size=8,color=\"black\") + \n",
    "     geom_text(data = jpn_label,label = \"***\",size=8,color=\"black\") + \n",
    "     geom_text(data = kor_label,label = \"***\",size=8,color=\"black\") +  \n",
    "     geom_text(data = nor_ns_label,label = \"ns\",size=4,color=\"black\",fontface = \"italic\") +  \n",
    "#      geom_text(data = pol_ns_label,label = \"ns\", size=4,color=\"black\",fontface = \"italic\") +  \n",
    "     geom_text(data = por_label,label = \"**\",size=8,color=\"black\") +  \n",
    "     geom_text(data = spa_ns_label,label = \"ns\",size=4,color=\"black\",fontface = \"italic\") + \n",
    "     geom_text(data = swe_label,label = \"ns\",size=4,color=\"black\",fontface = \"italic\") + \n",
    "     geom_text(data = tse_label,label = \"\", size=8, color=\"black\") +\n",
    "#      geom_text(data = zho_ns_label,label = \"ns\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "     # geom_text(data = zho_label,label = \"^\",size=8, color=\"black\") +\n",
    "     ylim(0, 250) +\n",
    "     labs(tag = \"A\",\n",
    "          y = \"Number of Unique Words\", x = \"\") +\n",
    "     theme_classic() +\n",
    "     scale_x_discrete(labels= xlabs) +\n",
    "     theme(text = element_text(size=16),\n",
    "           axis.text.x = element_text(vjust = 0.5, hjust=.5),\n",
    "           legend.title = element_blank(),\n",
    "           legend.background = element_rect(fill=alpha(\"white\",0.90),\n",
    "                                                            size=0, linetype=\"dotted\",\n",
    "                                                            colour = \"white\"),\n",
    "           legend.text=element_text(size=16))\n",
    "     ggsave(\"../figures/lexical_diversity_rand.pdf\", width = 11.7, height = 6.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i rand_lex_sumstats\n",
    "\n",
    "library('ggplot2')\n",
    "library('repr')\n",
    "options(repr.plot.width=6, repr.plot.height=12)\n",
    "\n",
    "xlabs <- c(\"C\", \"NC\")\n",
    "\n",
    "# ara_label <- data.frame(means=c(0),contingent = c(1.5),language=\"ara\") # no adult speech transcribed\n",
    "deu_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"German\")\n",
    "eng_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"English\")\n",
    "est_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Estonian\")\n",
    "fas_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Persian\")\n",
    "fra_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"French\")\n",
    "hrv_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Croatian\")\n",
    "jpn_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Japanese\")\n",
    "kor_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Korean\")\n",
    "# nor_label <- data.frame(means=c(5.8),contingent = c(1.5),Language_name=\"Norwegian\")\n",
    "nor_ns_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Norwegian\")\n",
    "# pol_ns_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Polish\")\n",
    "por_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Portuguese\")\n",
    "spa_ns_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Spanish\")\n",
    "swe_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Swedish\")\n",
    "tse_label <- data.frame(sums=c(247),contingent = c(1.5),Language_name=\"Tseltal\")\n",
    "# zho_label <- data.frame(sums=c(235),contingent = c(1.5),Language_name=\"Mandarin\")\n",
    "# zho_ns_label <- data.frame(means=c(6),contingent = c(1.5),Language_name=\"Mandarin\")\n",
    "\n",
    "\n",
    "p <- ggplot(rand_lex_sumstats, aes(x = contingent, y = sums, color = Language_name)) +\n",
    "     stat_summary(fun.y=mean, geom=\"point\", shape=19, size=1.75) + \n",
    "     stat_summary(fun.data = mean_se, geom = \"errorbar\", size=1.25, width = .4) +\n",
    "     facet_wrap(. ~ Language_name,ncol = 7) + \n",
    "     geom_text(data = deu_label,label = \"***\",size=6,color=\"black\") + \n",
    "     geom_text(data = eng_label,label = \"***\",size=6,color=\"black\") +  \n",
    "     geom_text(data = est_label,label = \"**\",size=6,color=\"black\") +  \n",
    "     geom_text(data = fas_label,label = \"*\",size=6, color=\"black\") +\n",
    "     geom_text(data = fra_label,label = \"***\",size=6,color=\"black\") +  \n",
    "     geom_text(data = hrv_label,label = \"***\",size=6,color=\"black\") + \n",
    "     geom_text(data = jpn_label,label = \"***\",size=6,color=\"black\") + \n",
    "     geom_text(data = kor_label,label = \"***\",size=6,color=\"black\") +  \n",
    "     geom_text(data = nor_ns_label,label = \"ns\",size=3,color=\"black\",fontface = \"italic\") +  \n",
    "     geom_text(data = por_label,label = \"**\",size=6,color=\"black\") +  \n",
    "     geom_text(data = spa_ns_label,label = \"ns\",size=3,color=\"black\",fontface = \"italic\") + \n",
    "     geom_text(data = swe_label,label = \"ns\",size=3,color=\"black\",fontface = \"italic\") + \n",
    "     geom_text(data = tse_label,label = \"\",size=6,color=\"black\") + \n",
    "     # geom_text(data = zho_label,label = \"^\",size=3, color=\"black\") +\n",
    "     ylim(0, 250) +\n",
    "     labs(tag = \"A\",\n",
    "          y = \"Number of Unique Words\", x = \"\") +\n",
    "     theme_classic() +\n",
    "     scale_x_discrete(labels= xlabs) +\n",
    "     theme(text = element_text(size=11.5),\n",
    "           axis.text.x = element_text(vjust = 0.5, hjust=0.5),\n",
    "           legend.position=\"none\")\n",
    "     ggsave(\"../figures/figure_2_A.pdf\", width = 11.5, height = 4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886fdfa",
   "metadata": {},
   "source": [
    "#### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95fc6e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`summarise()` has grouped output by 'Language_name'. You can override using the\n",
      "`.groups` argument.\n",
      "\n",
      "\n",
      "|Language   |Contingent M (SD) |Non-Contingent M (SD) |\n",
      "|:----------|:-----------------|:---------------------|\n",
      "|Croatian   |87.52 (37.27)     |124.66 (43.81)        |\n",
      "|English    |45.79 (33.14)     |151.24 (57.08)        |\n",
      "|Estonian   |94.91 (42.34)     |164.27 (57.91)        |\n",
      "|French     |47.68 (32.34)     |115.8 (59.34)         |\n",
      "|German     |101.13 (39.57)    |167.61 (60.51)        |\n",
      "|Japanese   |70.81 (29.22)     |122.28 (39.83)        |\n",
      "|Korean     |120.54 (39.34)    |227.54 (55.18)        |\n",
      "|Norwegian  |43.93 (43.84)     |71.78 (68.68)         |\n",
      "|Persian    |102.83 (50.34)    |276.83 (110.92)       |\n",
      "|Portuguese |79.48 (39.38)     |118.87 (44.25)        |\n",
      "|Spanish    |110.03 (30.66)    |112.6 (30.7)          |\n",
      "|Swedish    |108 (33.93)       |146.75 (41.9)         |\n",
      "|Tseltal    |119.9 (76.61)     |186 (98.35)           |\n"
     ]
    }
   ],
   "source": [
    "%%R -i rand_lex_sumstats\n",
    "\n",
    "rand_lex_sumstats %>%\n",
    "    group_by(Language_name, contingent) %>%\n",
    "    summarize(mean = mean(sums),\n",
    "              sd = sd(sums)) %>%\n",
    "    pivot_wider(names_from = contingent, values_from = c(mean, sd)) %>%\n",
    "    select(Language_name, mean_contingent, sd_contingent, `mean_non-contingent`, `sd_non-contingent`) %>%\n",
    "    `colnames<-`(c(\"Language\", \"Contingent Mean\", \"Contingent SD\", \"Non-Contingent Mean\", \"Non-Contingent SD\")) %>%\n",
    "    mutate(across(where(is.numeric), ~ round(., 2))) %>%\n",
    "    unite(\"Contingent M (SD)\", \"Contingent Mean\", \"Contingent SD\", sep = \" (\", na.rm = TRUE) %>%\n",
    "    mutate(`Contingent M (SD)` = paste0(`Contingent M (SD)`, \")\")) %>%\n",
    "    unite(\"Non-Contingent M (SD)\", \"Non-Contingent Mean\", \"Non-Contingent SD\", sep = \" (\", na.rm = TRUE) %>%\n",
    "    mutate(`Non-Contingent M (SD)` = paste0(`Non-Contingent M (SD)`, \")\")) %>%\n",
    "    kbl(\"pipe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2657c43",
   "metadata": {},
   "source": [
    "----\n",
    "#### Statistical analyses\n",
    "\n",
    "By language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ac4a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_lex_sumstats = rand_lex_sumstats[['Language_name','sums','contingent','transcript_id','target_child_id','caregiver_type']] #, 'Year collected']]\n",
    "\n",
    "rand_dat_inc_cg_count = rand_dat_inc_cg[['Language_name','transcript_id','target_child_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2175fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc_cg_count.to_csv(\"../data/rand_dat_inc_cg_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ce845dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining with `by = join_by(Language_name)`\n"
     ]
    }
   ],
   "source": [
    "%%R -i rand_lex_sumstats -i rand_dat_inc_cg_count\n",
    "\n",
    "# vectors for rows to remove from lmer\n",
    "case_study <- c(\"Persian\") # only 1 target child analyzed\n",
    "\n",
    "case_study_cgtype_compare <- c(\"Korean\") # only 1 target child analyzed, varies in CG type\n",
    "\n",
    "no_cgtype_compare <- c(\"Portuguese\", \"Tseltal\") # only `Mother only`\n",
    "\n",
    "# single_tran <- c(\"Polish\") # only 1 transcript\n",
    "\n",
    "# nests of models\n",
    "lexdiv_nest1 <- rand_lex_sumstats %>%\n",
    "    filter(!Language_name %in% case_study) %>%\n",
    "    filter(!Language_name %in% no_cgtype_compare) %>%\n",
    "    filter(!Language_name %in% case_study_cgtype_compare) %>%\n",
    "    group_by(Language_name) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lmer(sums ~ contingent + caregiver_type +\n",
    "                                (1|target_child_id) +\n",
    "                                (1|transcript_id),\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\"))),\n",
    "           effect_size = map2(summary, fit, ~ eff_size(.x, sigma = sigma(.y), edf = df.residual(.y)))) %>%\n",
    "    select(Language_name, contrasts, effect_size) %>%\n",
    "    unnest(cols = c(contrasts)) %>%\n",
    "    mutate(effect_size = map(effect_size, ~ summary(.))) %>%\n",
    "    unnest() %>%\n",
    "    mutate(statistic = coalesce(`t.ratio`), .before = p.value) %>%\n",
    "    select (-c(`t.ratio`))\n",
    "\n",
    "lexdiv_nest2 <- rand_lex_sumstats %>%\n",
    "    filter(Language_name %in% case_study_cgtype_compare) %>%\n",
    "    group_by(Language_name) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lmer(sums ~ contingent + caregiver_type +\n",
    "                                (1|transcript_id),\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\"))),\n",
    "           effect_size = map2(summary, fit, ~ eff_size(.x, sigma = sigma(.y), edf = df.residual(.y)))) %>%\n",
    "    select(Language_name, contrasts, effect_size) %>%\n",
    "    unnest(cols = c(contrasts)) %>%\n",
    "    mutate(effect_size = map(effect_size, ~ summary(.))) %>%\n",
    "    unnest() %>%\n",
    "    mutate(statistic = coalesce(`t.ratio`), .before = p.value) %>%\n",
    "    select (-c(`t.ratio`))\n",
    "\n",
    "lexdiv_nest3 <- rand_lex_sumstats %>%\n",
    "    filter(Language_name %in% no_cgtype_compare) %>%\n",
    "    group_by(Language_name) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lmer(sums ~ contingent + \n",
    "                                (1|target_child_id) +\n",
    "                                (1|transcript_id),\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\"))),\n",
    "           effect_size = map2(summary, fit, ~ eff_size(.x, sigma = sigma(.y), edf = df.residual(.y)))) %>%\n",
    "    select(Language_name, contrasts, effect_size) %>%\n",
    "    unnest(cols = c(contrasts)) %>%\n",
    "    mutate(effect_size = map(effect_size, ~ summary(.))) %>%\n",
    "    unnest() %>%\n",
    "    mutate(statistic = coalesce(`t.ratio`), .before = p.value) %>%\n",
    "    select (-c(`t.ratio`))\n",
    "\n",
    "lexdiv_nest4 <- rand_lex_sumstats %>%\n",
    "    filter(Language_name %in% case_study) %>%\n",
    "    group_by(Language_name) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lmer(sums ~ contingent +\n",
    "                                (1|transcript_id),\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\"))),\n",
    "           effect_size = map2(summary, fit, ~ eff_size(.x, sigma = sigma(.y), edf = df.residual(.y)))) %>%\n",
    "    select(Language_name, contrasts, effect_size) %>%\n",
    "    unnest(cols = c(contrasts)) %>%\n",
    "    mutate(effect_size = map(effect_size, ~ summary(.))) %>%\n",
    "    unnest() %>%\n",
    "    mutate(statistic = coalesce(`t.ratio`), .before = p.value) %>%\n",
    "    select (-c(`t.ratio`))\n",
    "    \n",
    "# number of transcripts per language\n",
    "sample_size <- rand_dat_inc_cg_count %>%\n",
    "    group_by(Language_name) %>%\n",
    "    summarize(n = n_distinct(transcript_id))\n",
    "    \n",
    "# combine lmer summaries and correct p-values for multiple comparisons\n",
    "emms_all <- list(lexdiv_nest1, lexdiv_nest2, lexdiv_nest3, lexdiv_nest4) %>% \n",
    "    reduce(bind_rows) %>%\n",
    "    mutate(p.value = p.adjust(p.value, \"holm\", 13)) %>%\n",
    "    left_join(sample_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9cf6b09",
   "metadata": {},
   "source": [
    "format statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e695be65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|Language (n)    |Estimate (SE)  | Test statistic| Effect size|Adjusted p-value |\n",
      "|:---------------|:--------------|--------------:|-----------:|:----------------|\n",
      "|Croatian (58)   |-37.14 (3.93)  |          -9.44|       -1.77|<.0001           |\n",
      "|English (882)   |-105.5 (2.04)  |         -51.74|       -2.49|<.0001           |\n",
      "|Estonian (22)   |-69.36 (15.44) |          -4.49|       -1.40|0.0019           |\n",
      "|French (279)    |-68.35 (3.3)   |         -20.71|       -1.76|<.0001           |\n",
      "|German (38)     |-66.47 (7.94)  |          -8.38|       -1.95|<.0001           |\n",
      "|Japanese (160)  |-51.46 (3.37)  |         -15.25|       -1.71|<.0001           |\n",
      "|Korean (28)     |-107 (12.68)   |          -8.44|       -2.32|<.0001           |\n",
      "|Norwegian (28)  |-27.07 (10.18) |          -2.66|       -0.73|0.1658           |\n",
      "|Persian (12)    |-174 (26.31)   |          -6.61|       -2.82|0.0002           |\n",
      "|Portuguese (23) |-39.39 (9.32)  |          -4.23|       -1.27|0.0038           |\n",
      "|Spanish (30)    |-2.57 (5.96)   |          -0.43|       -0.11|1.0000           |\n",
      "|Swedish (16)    |-38.75 (13.15) |          -2.95|       -1.08|0.1169           |\n",
      "|Tseltal (10)    |-66.1 (30.69)  |          -2.15|       -1.02|0.7029           |\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "table_maker = function(data) { data %>%\n",
    "    select(Language_name, n, estimate, SE, statistic, effect.size, p.value) %>%\n",
    "    `colnames<-`(c(\"Language\", \"n\", \"Estimate\", \"SE\", \"Test statistic\", \"Effect size\", \"Adjusted p-value\")) %>%\n",
    "    mutate_at(vars(-c(`Adjusted p-value`,Language)), round,2) %>%\n",
    "    mutate(`Adjusted p-value` = format(round(`Adjusted p-value`,4),nsmall=4)) %>%\n",
    "    mutate(`Adjusted p-value` = gsub(\"0.0000\",\"<.0001\",`Adjusted p-value`)) %>%\n",
    "    unite(\"Estimate (SE)\", c('Estimate','SE'), sep=\" (\") %>%\n",
    "    mutate(`Estimate (SE)` = paste0(`Estimate (SE)`,\")\")) %>%\n",
    "    unite(\"Language (n)\", c('Language','n'), sep=\" (\") %>%\n",
    "    mutate(`Language (n)` = paste0(`Language (n)`,\")\")) %>%\n",
    "    arrange(`Language (n)`)\n",
    "    }\n",
    "\n",
    "lexdiv_stats_table <- table_maker(emms_all)\n",
    "\n",
    "kable(lexdiv_stats_table,\"pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed81e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "# add columns sample and measure and save\n",
    "\n",
    "lexdiv_stats_table %>%\n",
    "    mutate(sample = \"rand\",\n",
    "           measure = \"lexdiv\") %>%\n",
    "    write.csv(file = \"../data/rand_lexdiv_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "845da2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|Language_name |sign_cat |   n|      prop|\n",
      "|:-------------|:--------|---:|---------:|\n",
      "|Croatian      |+        |  53| 0.9137931|\n",
      "|English       |+        | 839| 0.9512472|\n",
      "|Estonian      |+        |  18| 0.8181818|\n",
      "|French        |+        | 245| 0.8781362|\n",
      "|German        |+        |  36| 0.9473684|\n",
      "|Japanese      |+        | 145| 0.9062500|\n",
      "|Korean        |+        |  25| 0.8928571|\n",
      "|Norwegian     |+        |  16| 0.5714286|\n",
      "|Persian       |+        |  12| 1.0000000|\n",
      "|Portuguese    |+        |  20| 0.8695652|\n",
      "|Spanish       |+        |  15| 0.5000000|\n",
      "|Swedish       |+        |  13| 0.8125000|\n",
      "|Tseltal       |+        |   8| 0.8000000|\n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "\n",
    "# ---- Sign test per language\n",
    "\n",
    "rand_lex_stats_wide <- rand_lex_sumstats %>%\n",
    "  spread(key = contingent, value = sums) %>%\n",
    "  mutate(diff = `non-contingent` - contingent,\n",
    "         sign_cat = case_when(\n",
    "           diff > 0 ~ \"+\",\n",
    "           diff < 0 ~ \"-\",\n",
    "           TRUE ~ \"0\")) %>%\n",
    "  group_by(Language_name) %>%\n",
    "  count(sign_cat) %>%\n",
    "  mutate(prop = n / sum(n)) %>%\n",
    "  filter(sign_cat == \"+\")\n",
    "\n",
    "rand_lex_stats_wide %>%\n",
    "  mutate(measure = \"Number of Unique Words\") %>%\n",
    "  write.csv(file = \"../data/rand_lexdiv_signtest.csv\")\n",
    "\n",
    "kbl(rand_lex_stats_wide,\"pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5270185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 13 Ã— 5\n",
      "   Language_name successes trials test_result `Adjusted p-value`\n",
      "   <chr>             <dbl>  <int> <list>      <chr>             \n",
      " 1 Croatian             53     58 <htest>     <.0001            \n",
      " 2 English             839    858 <htest>     <.0001            \n",
      " 3 Estonian             18     22 <htest>     0.0261            \n",
      " 4 French              245    275 <htest>     <.0001            \n",
      " 5 German               36     38 <htest>     <.0001            \n",
      " 6 Japanese            145    160 <htest>     <.0001            \n",
      " 7 Korean               25     28 <htest>     0.0002            \n",
      " 8 Norwegian            16     27 <htest>     1.0000            \n",
      " 9 Persian              12     12 <htest>     0.0039            \n",
      "10 Portuguese           20     23 <htest>     0.0039            \n",
      "11 Spanish              15     30 <htest>     1.0000            \n",
      "12 Swedish              13     16 <htest>     0.1064            \n",
      "13 Tseltal               8     10 <htest>     0.4375            \n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "\n",
    "sign_tests <- rand_lex_sumstats %>%\n",
    "  spread(key = contingent, value = sums) %>%\n",
    "  filter(!is.na(contingent) & !is.na(`non-contingent`)) %>%\n",
    "  mutate(diff = `non-contingent` - contingent,\n",
    "         success = if_else(diff > 0, 1, 0)) %>%\n",
    "  group_by(Language_name) %>%\n",
    "  summarize(successes = sum(success),\n",
    "            trials = n(),\n",
    "            .groups = 'drop')\n",
    "\n",
    "# Nest the data and run binom.test\n",
    "sign_tests %>%\n",
    "  mutate(\n",
    "    test_result = purrr::map2(successes, trials, ~binom.test(x = .x, n = .y)),\n",
    "    `Adjusted p-value` = map_dbl(test_result, 'p.value'),\n",
    "    `Adjusted p-value` = p.adjust(`Adjusted p-value`, \"holm\", 14),\n",
    "    `Adjusted p-value` = format(round(`Adjusted p-value`,4),nsmall=4),\n",
    "    `Adjusted p-value` = gsub(\"0.0000\",\"<.0001\",`Adjusted p-value`)\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c88dec2c",
   "metadata": {},
   "source": [
    "By play context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d686f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_lex_sumstats_contex = (rand_dat_inc_cg.groupby([\"context\",\"target_child_id\",\"transcript_id\",\"contingent\"])\n",
    "                                  .uniqueness\n",
    "                                  .agg([\"sum\"])\n",
    "                                  .reset_index())\n",
    "\n",
    "rand_lex_sumstats_contex =  rand_lex_sumstats_contex.rename({'sum': 'sums'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c0d0485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home: book reading', 'Home: other', 'Home: unstructured',\n",
       "       'Lab: interview/unstructured', 'Lab: unstructured',\n",
       "       'Other: unstructured', 'Unreported'], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_lex_sumstats_contex[\"context\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d061c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining with `by = join_by(context)`\n"
     ]
    }
   ],
   "source": [
    "%%R -i rand_lex_sumstats_contex\n",
    "\n",
    "# vectors for rows to remove from lmer\n",
    "single_tran <- c(\"Home: interview/unstructured\") # only 1 transcript\n",
    "\n",
    "contex_sample_size <- rand_lex_sumstats_contex %>%\n",
    "    group_by(context) %>%\n",
    "    summarize(n = n_distinct(transcript_id))\n",
    "\n",
    "lexdiv_contex_nest_1 <- rand_lex_sumstats_contex %>%\n",
    "    filter(!context %in% single_tran) %>%\n",
    "    group_by(context) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lmer(sums ~ contingent +\n",
    "                                (1|transcript_id),\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\"))),\n",
    "           effect_size = map2(summary, fit, ~ eff_size(.x, sigma = sigma(.y), edf = df.residual(.y)))) %>%\n",
    "    select(context, contrasts, effect_size) %>%\n",
    "    unnest(cols = c(contrasts)) %>%\n",
    "    mutate(effect_size = map(effect_size, ~ summary(.))) %>%\n",
    "    unnest() %>%\n",
    "    mutate(statistic = coalesce(`t.ratio`), .before = p.value) %>%\n",
    "    select (-c(`t.ratio`))\n",
    "    \n",
    "lexdiv_contex_nest_2 <- rand_lex_sumstats_contex %>%\n",
    "    filter(context %in% single_tran) %>%\n",
    "    group_by(context) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lm(sums ~ contingent,\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\")))) %>%\n",
    "    select(context, contrasts) %>%\n",
    "    unnest(cols = c(contrasts))\n",
    "\n",
    "# combine lmer summaries and correct p-values for multiple comparisons\n",
    "context_emms_all <- list(lexdiv_contex_nest_1, lexdiv_contex_nest_2) %>% \n",
    "    reduce(bind_rows) %>%\n",
    "    mutate(p.value = p.adjust(p.value, \"holm\", 7)) %>%\n",
    "    left_join(contex_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b184790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|Play context (n)                  |Estimate (SE)  | Test statistic| Effect size|Adjusted p-value |\n",
      "|:---------------------------------|:--------------|--------------:|-----------:|:----------------|\n",
      "|Home: book reading (28)           |-107 (12.81)   |          -8.36|       -2.27|<.0001           |\n",
      "|Home: other (20)                  |-46.8 (5.34)   |          -8.77|       -2.84|<.0001           |\n",
      "|Home: unstructured (900)          |-72.45 (1.93)  |         -37.61|       -1.78|<.0001           |\n",
      "|Lab: interview/unstructured (368) |-141.7 (2.85)  |         -49.76|       -3.74|<.0001           |\n",
      "|Lab: unstructured (26)            |-62.58 (10.63) |          -5.88|       -1.66|<.0001           |\n",
      "|Other: unstructured (138)         |-60.58 (3.67)  |         -16.50|       -1.99|<.0001           |\n",
      "|Unreported (106)                  |-35.99 (6.23)  |          -5.78|       -0.80|<.0001           |\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "table_maker = function(data) { data %>%\n",
    "    select(context, n, estimate, SE, statistic, effect.size, p.value) %>%\n",
    "    `colnames<-`(c(\"Play context\", \"n\", \"Estimate\", \"SE\", \"Test statistic\", \"Effect size\", \"Adjusted p-value\")) %>%\n",
    "    mutate_at(vars(-c(`Adjusted p-value`,`Play context`)), round,2) %>%\n",
    "    mutate(`Adjusted p-value` = format(round(`Adjusted p-value`,4),nsmall=4)) %>%\n",
    "    mutate(`Adjusted p-value` = gsub(\"0.0000\",\"<.0001\",`Adjusted p-value`)) %>%\n",
    "    unite(\"Estimate (SE)\", c('Estimate','SE'), sep=\" (\") %>%\n",
    "    mutate(`Estimate (SE)` = paste0(`Estimate (SE)`,\")\")) %>%\n",
    "    unite(\"Play context (n)\", c(`Play context`,'n'), sep=\" (\") %>%\n",
    "    mutate(`Play context (n)` = paste0(`Play context (n)`,\")\")) %>%\n",
    "    arrange(`Play context (n)`)\n",
    "    }\n",
    "    \n",
    "lexdiv_context_stats_table <- table_maker(context_emms_all)\n",
    "\n",
    "kable(lexdiv_context_stats_table, \"pipe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29977c2d",
   "metadata": {},
   "source": [
    "By context, dropping English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f298e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop English\n",
    "rand_dat_inc_cg_no_eng = rand_dat_inc_cg[rand_dat_inc_cg[\"Language_name\"] != \"English\"]\n",
    "\n",
    "rand_lex_sumstats_contex_no_eng = (rand_dat_inc_cg_no_eng.groupby([\"context\",\"target_child_id\",\"transcript_id\",\"contingent\"])\n",
    "                                  .uniquenss\n",
    "                                  .agg([\"sum\"])\n",
    "                                  .reset_index())\n",
    "\n",
    "rand_lex_sumstats_contex_no_eng =  rand_lex_sumstats_contex_no_eng.rename({'sum': 'sums'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81956fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see help('isSingular')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining with `by = join_by(context)`\n"
     ]
    }
   ],
   "source": [
    "%%R -i rand_lex_sumstats_contex_no_eng\n",
    "\n",
    "# # vectors for rows to remove from lmer\n",
    "single_tran <- c(\"Home: interview/unstructured\") # only 1 transcript\n",
    "\n",
    "contex_sample_size_no_eng <- rand_lex_sumstats_contex_no_eng %>%\n",
    "    group_by(context) %>%\n",
    "    summarize(n = n_distinct(transcript_id))\n",
    "\n",
    "lexdiv_contex_nest_1_no_eng <- rand_lex_sumstats_contex_no_eng %>%\n",
    "    filter(!context %in% single_tran) %>%\n",
    "    group_by(context) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lmer(sums ~ contingent +\n",
    "                                (1|transcript_id),\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\"))),\n",
    "           effect_size = map2(summary, fit, ~ eff_size(.x, sigma = sigma(.y), edf = df.residual(.y)))) %>%\n",
    "    select(context, contrasts, effect_size) %>%\n",
    "    unnest(cols = c(contrasts)) %>%\n",
    "    mutate(effect_size = map(effect_size, ~ summary(.))) %>%\n",
    "    unnest() %>%\n",
    "    mutate(statistic = coalesce(`t.ratio`), .before = p.value) %>%\n",
    "    select (-c(`t.ratio`))\n",
    "\n",
    "lexdiv_contex_nest_2_no_eng <- rand_lex_sumstats_contex_no_eng %>%\n",
    "    filter(context %in% single_tran) %>%\n",
    "    group_by(context) %>%\n",
    "    nest() %>%\n",
    "    mutate(fit = map(data, ~ lm(sums ~ contingent,\n",
    "                                data = .,\n",
    "                                REML= FALSE)),\n",
    "           summary = map(fit, ~ emmeans(., \"contingent\")),\n",
    "           contrasts = map(summary, ~ summary(contrast(., method = \"pairwise\")))) %>%\n",
    "    select(context, contrasts) %>%\n",
    "    unnest(cols = c(contrasts))  %>%\n",
    "    rename(statistic = `t.ratio`)\n",
    "    \n",
    "# combine lmer summaries and correct p-values for multiple comparisons\n",
    "context_no_eng_emms_all <- list(lexdiv_contex_nest_1_no_eng, lexdiv_contex_nest_2_no_eng) %>% \n",
    "    reduce(bind_rows) %>%\n",
    "    mutate(p.value = p.adjust(p.value, \"holm\", 3)) %>%\n",
    "    left_join(contex_sample_size_no_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c1cc4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|Play context (n)                 |Estimate (SE)   | Test statistic| Effect size|Adjusted p-value |\n",
      "|:--------------------------------|:---------------|--------------:|-----------:|:----------------|\n",
      "|Home: book reading (28)          |-116.71 (10.89) |         -10.72|       -2.92|<.0001           |\n",
      "|Home: interview/unstructured (1) |-13 (NaN)       |            NaN|          NA|NaN              |\n",
      "|Home: unstructured (560)         |-64.37 (2.12)   |         -30.41|       -1.83|<.0001           |\n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "\n",
    "table_maker = function(data) { data %>%\n",
    "    select(context, n, estimate, SE, statistic, effect.size, p.value) %>%\n",
    "    `colnames<-`(c(\"Play context\", \"n\", \"Estimate\", \"SE\", \"Test statistic\", \"Effect size\", \"Adjusted p-value\")) %>%\n",
    "    mutate_at(vars(-c(`Adjusted p-value`,`Play context`)), round,2) %>%\n",
    "    mutate(`Adjusted p-value` = format(round(`Adjusted p-value`,4),nsmall=4)) %>%\n",
    "    mutate(`Adjusted p-value` = gsub(\"0.0000\",\"<.0001\",`Adjusted p-value`)) %>%\n",
    "    unite(\"Estimate (SE)\", c('Estimate','SE'), sep=\" (\") %>%\n",
    "    mutate(`Estimate (SE)` = paste0(`Estimate (SE)`,\")\")) %>%\n",
    "    unite(\"Play context (n)\", c(`Play context`,'n'), sep=\" (\") %>%\n",
    "    mutate(`Play context (n)` = paste0(`Play context (n)`,\")\")) %>%\n",
    "    arrange(`Play context (n)`)\n",
    "    }\n",
    "    \n",
    "lexdiv_context_stats_table_no_eng <- table_maker(context_no_eng_emms_all)\n",
    "\n",
    "kable(lexdiv_context_stats_table_no_eng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
